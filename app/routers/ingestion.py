from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from datetime import datetime
import os
import json
import uuid
import requests
import tempfile
import shutil

# On importe nos outils Firebase configur√©s
try:
    from app.firebase_config import db, bucket
except ImportError:
    from firebase_config import db, bucket

router = APIRouter()

class IngestionRequest(BaseModel):
    user_id: str
    file_url: str
    file_type: str

def process_file_task(req: IngestionRequest):
    """
    T√¢che en arri√®re-plan pour ne pas bloquer l'API.
    """
    temp_dir = tempfile.mkdtemp()
    local_path = os.path.join(temp_dir, f"input.{req.file_type}")
    
    try:
        print(f"üì• Downloading file for user {req.user_id}...")
        # 1. T√©l√©charger le fichier depuis l'URL Firebase (sign√©e ou publique)
        with requests.get(req.file_url, stream=True) as r:
            r.raise_for_status()
            with open(local_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        
        # 2. Traitement / Conversion (Simulation pour l'instant)
        # ICI TU METTRAS TA LOGIQUE DE PARSING (MDB, SI2S...)
        print("‚öôÔ∏è Processing file...")
        
        # Simulation d'un r√©sultat JSON
        result_data = {
            "project_name": "Imported Project",
            "source_file": req.file_url,
            "processed_at": datetime.now().isoformat(),
            "transformers": [{"name": "TX1", "sn_kva": 1000}], # Exemple
            "plans": []
        }
        
        # Si c'est un vrai JSON upload√©, on le lit
        if req.file_type == 'json':
            try:
                with open(local_path, 'r') as f:
                    result_data = json.load(f)
            except: pass

        # 3. Sauvegarder le JSON r√©sultat dans Storage
        result_filename = f"processed/{req.user_id}/{uuid.uuid4()}.json"
        blob = bucket.blob(result_filename)
        blob.upload_from_string(json.dumps(result_data), content_type='application/json')
        print(f"üíæ Result uploaded to {result_filename}")

        # 4. Cr√©er la fiche dans Firestore (C'est √áA qui fait appara√Ætre la ligne sur le site)
        doc_ref = db.collection('users').document(req.user_id).collection('configurations').document()
        doc_ref.set({
            'created_at': firestore.SERVER_TIMESTAMP,
            'source_type': req.file_type,
            'original_name': 'Uploaded File',
            'processed': True,
            'is_large_file': True,
            'storage_path': result_filename, # Lien vers le JSON complet
            'raw_data': None # On ne met pas tout le JSON ici pour ne pas alourdir Firestore
        })
        print("‚úÖ Firestore document created!")

    except Exception as e:
        print(f"‚ùå Error processing file: {e}")
        # Optionnel : Mettre √† jour Firestore avec un statut d'erreur
    finally:
        shutil.rmtree(temp_dir)

@router.post("/process")
async def start_ingestion(req: IngestionRequest, background_tasks: BackgroundTasks):
    """
    Endpoint appel√© par le Frontend apr√®s l'upload.
    """
    # On lance le traitement en t√¢che de fond pour r√©pondre tout de suite au Frontend
    background_tasks.add_task(process_file_task, req)
    return {"status": "started", "message": "Processing started in background"}

@router.get("/download-all/{format}")
async def download_all(format: str, user_id: str):
    # TODO: Impl√©menter la logique ZIP ici
    return {"message": "Not implemented yet, but connected!"}
